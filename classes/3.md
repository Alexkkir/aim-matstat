$$
\newcommand{\horline}{\noindent\rule{\textwidth}{0.4pt}}
\renewcommand{\line}{\\ \hline}

\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\R}{{\mathbb{R}}}
\renewcommand{\C}{{\mathbb{C}}}
\renewcommand{\P}{{\mathbb{P}}}
\newcommand{\F}{{\mathbb{F}}}
\newcommand{\PP}{\text{П}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\D}{{\mathbb{D}}}
\newcommand{\I}{{\mathbb{I}}}

\renewcommand{\O}{{\Omega}}
\newcommand{\G}{{\Gamma}}
\renewcommand{\a}{{\alpha}}
\renewcommand{\b}{{\beta}}
\newcommand{\g}{{\gamma}}
\renewcommand{\d}{{\delta}}
\newcommand{\e}{{\varepsilon}}
\renewcommand{\f}{{\varphi}}
\newcommand{\s}{{\sigma}}
\newcommand{\w}{{\omega}}
\renewcommand{\r}{{\rho}}
\renewcommand{\l}{{\lambda}}
\renewcommand{\k}{{\kappa}}
\renewcommand{\L}{{\Lambda}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\green}[1]{{\color{green} #1}}
\newcommand{\purple}[1]{{\color{purple} #1}}
\newcommand{\orange}[1]{{\color{orange} #1}}
\renewcommand{\bold}[1]{\textbf{#1}}
\renewcommand{\vec}[1]{{\overrightarrow{#1}}}
% \renewcommand{\inf}{{\infty}}

% \newenvironment{eqn}{ \begin{align} \begin{split} }{ \end{split} \end{align} }

% \newenvironment{eq}{\begin{align*}}{\end{align*}}

\newcommand{\arr}[1]{{\left[ \begin{array}{*{20}{>{\centering\arraybackslash}c}} #1 \end{array} \right]}}


\renewcommand{\table}[3]{\begin{center}\begin{tabular}{|*{#1}{>{\centering\arraybackslash}p{#2\textwidth}|}} \hline #3 \hline \end{tabular}\end{center}}

\newcommand{\tablea}[2]{\begin{center}\begin{tabular}{|*{#1}{>{\centering\arraybackslash}c|}} \hline #2 \hline \end{tabular}\end{center}}
  

% \forall \e > 0 \ \exists \d > 0 : \ \forall x \in O_\delta(x_0) \

\renewcommand{\leadsto}{{\ \Longrightarrow \ }}
\newcommand{\sat}{{\mapsto}}
\newcommand{\cons}{{\ \Rightarrow \ }}
\newcommand{\then}{{\ \Rightarrow \ }}
\newcommand{\hence}{{\ \Rightarrow \ }}


\newcommand{\thesame}{{\ \Leftrightarrow \ }}
\newcommand{\<}{{\leqslant}}
\renewcommand{\>}{{\geqslant}}
% \let\oldline\|
% \renewcommand{\#}{\oldline}
% \renewcommand{\|}{\Big|}
\renewcommand{\=}{\equiv}
\newcommand{\9}{\Big(}
\newcommand{\0}{\Big)}
\newcommand{\calc}[2]{\Bigg|_{#1}^{#2}}
\newcommand{\dx}{\, dx}
\newcommand{\dt}{\, dt}
\newcommand{\dy}{\, dy}
\newcommand{\dz}{\, dz}
\newcommand{\du}{\, du}
\newcommand{\dv}{\, dv}
\newcommand{\df}{\, d \f}
\newcommand{\bs}{\Big[}
\newcommand{\be}{\Big]}

\renewcommand{\eq}{= \\ =}
\newcommand{\shift}[1]{\Big\{ #1 \Big\}}
\newcommand{\set}[1]{\{ #1 \}}
\renewcommand{\Im}{{\operatorname{Im}}}
\renewcommand{\Re}{{\operatorname{Re}}}
\newcommand{\im}{{\operatorname{Im}}}
\newcommand{\re}{{\operatorname{Re}}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\con}{\operatorname{con}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\ifff}{\operatorname{iff}}
\newcommand{\st}{\operatorname{s.t.}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\prox}{\operatorname{prox}}

% \let\oldelse\else
\newcommand{\otherwise}{\text{иначе}}

\renewcommand{\exp}[1]{\operatorname{exp} \{ #1 \}}
\newcommand{\eexp}[1]{\operatorname{exp} \Big \{ #1 \Big \}}
$$

# Непараметрические методы оценки плотности

Пусть есть $$X_1, \dots X_n \sim P$$ - множество абсолютно непр распрееление

Задача - оценить p: $\mathbb{R} \to \mathbb{R}_+, \ \int p(x) dx = 1$

Зачем?

1. $p \to E, Var$

2. кластеризация

   ![image-20230320185602761](/home/alexkkir/.config/Typora/typora-user-images/image-20230320185602761.png)

   В книге Wasserman “All of nonparam statistics” есть пример с оцениваем расстоянием до космических объектов. Есть сайт [sdss.org](http://sdss.org). С помощь. специальной методики оцениваются расстояние от земли до далеких небесных тел. После этого можно разделить тела на калактики. Это можно сделать построив график плотности. Удалось найти порядка тысячи галатик.

Непараметрическая оценка функции распредениея (эмпирическая)
$$
 F_n(x) = {1 \over n} \sum \mathbb{I} \{ x_i < x \} 
$$


она оч хорошая на самом деле.

Возникает идея:
$$
\hat p_n(x) = {d \over dx} \hat F_n(x)
$$


Другая идея - гистрограмма. В математике немного отличается от общепринятого представления
$$
P \{ a < X \leq b \} = F(b) - F(a) = \int_a^b p(u) du
$$
Нужно аппроксимировать интеграл:

<img src="/home/alexkkir/.config/Typora/typora-user-images/image-20230320185828141.png" alt="image-20230320185828141" style="zoom:100%;" />

Тогда
$$
(...) = p(x) (b - a), \ x \in [a, b]
$$
(аппроксимировали прямоугольником)

Т.к.
$$
P \{ x - {h \over 2} < x \< x + {h \over 2} \} = p(x) h
$$
то
$$
p(x) = {P \{ x - {h \over 2} < x \< x + {h \over 2}\} \over h} \approx  {\# \{i: x - {h \over 2} < x_i \< x + {h \over 2}\} \over nh}
$$
Значит, разбиваем гистограмму на интервалы бины и для каждого считаем статистики
$$
B_1, \dots B_n - \text{bins} \\
|B_1| = \dots = |B_m| = h - \text{bandwidth}  \\
C_1, \dots, C_n - \text{центры} \\
B_i = [C_i  {h \over 2}, C_i + {h \over 2}] \\
\boxed{\hat p_n(x) = {\# \{ i: x_i \in B_j\} \over nh}, \ x \in B_j}
$$

## Как оценить качество?

Bias-variance decomposition tradeoff

Первое что приходит в ум - посчитать ошибку
$$
\E [\hat p_n(x) - p(x))^2] = MSE(\hat p_n(x))
$$
Недостаток - зависит от $x$. Более устойчивая вещь:
$$
\int MSE(\hat p_n(x)) dx = MISE(\hat p_n(x))
$$
— mean integer square error

Можно разложить MSE так:
$$ { }
MSE(\hat p_n(x)) = (Bias(\hat p_n(x)))^2 + Var \hat p_n(x) \\
MISE(\hat p_n(x)) = \int (Bias(\hat p_n(x)))^2 + \int Var \hat p_n(x)
$$
В чем компромисс?

**Теорема**. Если
$$
\int (p'(x))^2 \dx < \infty
$$
то
$$
MISE(\hat p_n(x)) = ({h^2 \over 12} \int (p'(x))^2 \dx + {1 \over nh})(1 + \bar o(1)), \ n \to \infty
$$
Проанализируем

* первое слагаемое растет как парабола (это bias)
* второе слагаемое убывает как гипербола (это дисперсия)
* есть минимум

Резюме: надо пользоваться MISE

## Как выбирается параметр $h$ в R?

Можно просты вычислить ноль производной функции выше:
$$
{d \over dh} AMISE(h) = {d \over dh} ({h^2 \over 12} \int (p'(x))^2 \dx + {1 \over nh}) \\
h_{opt} = n^{-1/3} ({6 \over \int p'(x)^2\dx})^{1/3}
$$
AMISE - асимптотическая MISE

На самом деле можно было не вычислять производную.

1. $AMISE(h) = f(n) h^{k_1} + f_2(n) h^{k_2}, \ k_1 = 2, k^2 = -1$

   Если продифференцировать то получим, что $h_{opt} = C \cdot ({f_2(n) \over f_1(n)})^{{1 \over k_1 + k_2}}$

   Но есть фокус: можно просто приравнять порядки:
   $$
   f(n) h^{k_1} =  f_2(n) h^{k_2} \hence \tilde h = h_{opt}
   $$
   Это свойство верно и для дальнейших методов

   Но мы не можем вычислить эту оценку, т.к. не знаем интеграл. Оценка выше называется оракульной

2. Правило Скотта (Scott’s rule):
   $$
   p(x) = {1 \over \sqrt {2 \pi} \sigma} \exp{-x^2 / 2 \sigma^2}, \ N(0, \sigma^2)
   $$

​		Отсюда $$h_{opt} = n^{-1/3} (24 \sqrt \pi)^{1/3} \sigma \approx 3.5$$. Идея: 
$$
\hat h_{opt} = n^{-1/3} 3.5 \hat \sigma_n, \ \hat \sigma_n = {1 \over n} \sum (x_i - \bar x)^2
$$

3. Правило Фридмана-Дьякони (FD)

$$
\hat h_{opt} = n^{-1 / 3} \cdot 2 \cdot \text{IQR} (x_1, \dots, x_n)$$
$$

4. Метод Стерджеса

   Пусть у нас Биноминальное распределение: $$\xi = \mu_1 + \dots \mu_n, \ P(\xi = k) = C_n^k p^k (1-p)^{m - k}$$. Нам нужно приблизить эту картинку к нормальному распределению. Если у нас есть $$n$$ наблюдений, то оптимально взять $$m = \log_2 n$$. 

> В языке R все формулы считаются по немного другим формулам

В R используется функция pretty, которая делает число “красивым”. Число красивое, если первый знак после запятой это 2, 4, 6, 8, 5 или число вообще целое

# Ядерная оценка плотности

density(X)

Kernel density esimate

Kernel $$K: \R \to \R_+, \ \int K(x) \dx = 1$$, $$K$$ - четная

* boxcar kernel: $$1 / 2 \mathbb{I} \{ |x| < 1\}$$
* triangle kernel: $$(1 - |x|) \I \{ |x| < 1 \}$$
* Epanechnikov kernel: $$3/4 (1-x^2) \I \{ |x| < 1 \}$$
* Gaussian kernel: $${1 \over \sqrt{2 \pi}} \exp{-x^2 / 2}$$

Kernel density estimate:
$$
\hat p_n(x) = {1 \over nh} \sum K({x - X_i \over h})
$$
Можно заметить, что 
$$
(\dots) \neq 0 \iff |{x - X_i \over h}| < 1
$$
**Теорема**. Если $$\int (p''(x))^2 \dx < \infty$$, то
$$
\text{MISE} = \9 {1 \over 4} h^4 \int (p''(x))^2 \dx  \9 \int x^2 K(x) \dx \0^4 + {1 \over nh} \int K^2(x) \dx \0 (1 + o(1))
$$
Приравниваем порядки: $$h^4 = 1 / nh \hence h_{opt} \sim n^{-1 / 5}$$.

А с гистограммой мы получили $$h_{opt} \sim n^{-1/3}$$. Почему другой порядок?

Теорема. Пусть $$X_1, \dots X_n \sim \Phi_m = \set{\text{abs. cont.}, \ \int (p^{(m)}(x))^2}$$

Тогда 
$$
\sup_{p \in \Phi_m} \text{MISE}(\hat p_n) \> C \cdot n^{-{2m \over 2m + 1}}
$$
Соответственно, для $\Phi_1$ у нас порядок $n^{-2/3}$, для $\Phi_2$ порядок $n^{-4/5}$. То есть гистограмная оценка оптимальна в классе $\Phi_1$, ядерная - в классе $\Phi_2$. 

Итоговая оценка имеет вид

![image-20230320204641347](/home/alexkkir/.config/Typora/typora-user-images/image-20230320204641347.png)

Можно предположить нормальность распределения и тогда получим оценку

![image-20230320204737502](/home/alexkkir/.config/Typora/typora-user-images/image-20230320204737502.png)

В R используется nrd для оценки дисперсии:

![image-20230320204837053](/home/alexkkir/.config/Typora/typora-user-images/image-20230320204837053.png)

Есть еще аналогичное “nrd 0”. Вместо оценки $(4/5)^{1/5}$ берут 0.9 (чисто эмпирически).

### Как выбрать ядро?

При фиксированном $h$ мы получаем функционал от ядра, который можно оптимимзировать. И для этой задачи даже есть решение

![image-20230320210229990](/home/alexkkir/.config/Typora/typora-user-images/image-20230320210229990.png)

  

Ядро Епанечникова - самое лучшее. Но на самом деле выбор ядра не очень важно. Определим эффективность:
$$
\text{eff}(K) = {I(K_{ep}) \over I(K)} \< 1
$$
Эта штука фигурирует в формуле. На самом деле, почти для всех ядер эффективность близка к 1, просто у Епанечникова она максимальная.

## Почему ядро Епанечникова на самом деле не оптимально?

1. Выше была теорема:

   ![image-20230320210645172](/home/alexkkir/.config/Typora/typora-user-images/image-20230320210645172.png)

   У ядра Епанечникова константа максимальна

2. Посмотрим на ядра в более общем смысле: как функцию из $\R$ в $\R$.

   ![image-20230320210901211](/home/alexkkir/.config/Typora/typora-user-images/image-20230320210901211.png)

   В чем смысл - не понял.

   Можно возразить: раньше была хорошая функция, а теперь у нас некрасивая, но более оптимальная. Но есть утверждение:

![image-20230320211130591](/home/alexkkir/.config/Typora/typora-user-images/image-20230320211130591.png)

![image-20230320211151982](/home/alexkkir/.config/Typora/typora-user-images/image-20230320211151982.png)

​	

# EM алгоритм

* единственный параметрический алгоритм оценки плотности, который хорошо работает

EM - excpectation maximization

Рассмотрим смесь двух нормальных распределений:

![image-20230320211511395](/home/alexkkir/.config/Typora/typora-user-images/image-20230320211511395.png)

Параметры и задача:

![image-20230320211543323](/home/alexkkir/.config/Typora/typora-user-images/image-20230320211543323.png)

Первое что приходит в голову - метод максимального правдоподобия

![image-20230320211612369](/home/alexkkir/.config/Typora/typora-user-images/image-20230320211612369.png)

Это сложная оптимизационная задача. Можно использовать метода Бидоля-Равсона. Но это не очень.

### Идея

Введем фиктивные (латентные) переменные:

![image-20230320211751591](/home/alexkkir/.config/Typora/typora-user-images/image-20230320211751591.png)

У нас распределение является смесью двух других

![image-20230320211847502](/home/alexkkir/.config/Typora/typora-user-images/image-20230320211847502.png)

### Отступление: как моделировать величину из смеси распределений

![image-20230320211936473](/home/alexkkir/.config/Typora/typora-user-images/image-20230320211936473.png)

Как доказать? Считаем функцию распределения:

![image-20230320212029354](/home/alexkkir/.config/Typora/typora-user-images/image-20230320212029354.png)

Дифференцируем:

![image-20230320212106479](/home/alexkkir/.config/Typora/typora-user-images/image-20230320212106479.png)

### Возвращаемся

Итак, у нас есть $Y_i$:

![image-20230320212312762](/home/alexkkir/.config/Typora/typora-user-images/image-20230320212312762.png)

Тогда

![image-20230320212340073](/home/alexkkir/.config/Typora/typora-user-images/image-20230320212340073.png)

Дифференцируем и приводим к красивому виду:

![image-20230320212442367](/home/alexkkir/.config/Typora/typora-user-images/image-20230320212442367.png)

Записываем лог-правдободобие:

![image-20230320212709026](/home/alexkkir/.config/Typora/typora-user-images/image-20230320212709026.png)

 

![image-20230320212759605](/home/alexkkir/.config/Typora/typora-user-images/image-20230320212759605.png)

Но как отсюда вытащить оценку? Запишем $\log L_\theta (x_1, \dots x_n, y_1, \dots y_n)$ и вместо $y_i$ подставим $Y_i$ - случайные величины. А потом возьмем матожидание и максимизируем по $\theta$Э

![image-20230320213014511](/home/alexkkir/.config/Typora/typora-user-images/image-20230320213014511.png)

### Как выглядит алгоритм

1. Выбираем $\theta^{(0)}$

2. $\E$-step

   ![image-20230320213551531](/home/alexkkir/.config/Typora/typora-user-images/image-20230320213551531.png)

   Тут

   ![image-20230320213658236](/home/alexkkir/.config/Typora/typora-user-images/image-20230320213658236.png)

   Вспомним формулу Байеса:

   ![image-20230320213734042](/home/alexkkir/.config/Typora/typora-user-images/image-20230320213734042.png)

   Тогда

   <img src="/home/alexkkir/.config/Typora/typora-user-images/image-20230320213744850.png" alt="image-20230320213744850" style="zoom:80%;" />

3. M-step:
4. Вернуться к шагу 1

Зачем нужны были латентные переменные? Они постоянно убираются

### Почему это работает?

Изначальная идея - построить оценку лог правдободобия

![image-20230320214007471](/home/alexkkir/.config/Typora/typora-user-images/image-20230320214007471.png)

Преобразуем через условные вероятности:

![image-20230320214043422](/home/alexkkir/.config/Typora/typora-user-images/image-20230320214043422.png)

и распишем логорифм:

![image-20230320214113614](/home/alexkkir/.config/Typora/typora-user-images/image-20230320214113614.png)

Т.к. $y_i$ случайные, можно взять любые. Возьмем случайные $y_i$, то есть $Y_i$ и используем матожидание:

![image-20230320214432634](/home/alexkkir/.config/Typora/typora-user-images/image-20230320214432634.png)



Посмотрим, как у нас меняется логправдоподобие после итерации:

![image-20230320214637576](/home/alexkkir/.config/Typora/typora-user-images/image-20230320214637576.png)

Второе слагаемое это KLD:

![image-20230320214658235](/home/alexkkir/.config/Typora/typora-user-images/image-20230320214658235.png)

Получается, мы с каждый шагом работы EM алгоритма увеличиваем логорифм правдободия, а значит и приближаемся к ней. Возможно, мы попадем в локальный минимум, но в любом случае движемся мы в правильном направлении
