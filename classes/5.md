$$
\newcommand{\horline}{\noindent\rule{\textwidth}{0.4pt}}
\renewcommand{\line}{\\ \hline}

\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\R}{{\mathbb{R}}}
\renewcommand{\C}{{\mathbb{C}}}
\renewcommand{\P}{{\mathbb{P}}}
\newcommand{\F}{{\mathbb{F}}}
\newcommand{\PP}{\text{П}}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\D}{{\mathbb{D}}}
\newcommand{\I}{{\mathbb{I}}}

\newcommand{\Pp}{\mathcal{P}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\Hh}{\mathcal{H}}
\newcommand{\Aa}{\mathcal{A}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Xx}{\mathcal{X}}

\renewcommand{\O}{{\Omega}}
\newcommand{\G}{{\Gamma}}
\renewcommand{\a}{{\alpha}}
\renewcommand{\b}{{\beta}}
\newcommand{\g}{{\gamma}}
\renewcommand{\d}{{\delta}}
\newcommand{\e}{{\varepsilon}}
\renewcommand{\f}{{\varphi}}
\newcommand{\s}{{\sigma}}
\newcommand{\w}{{\omega}}
\renewcommand{\r}{{\rho}}
\renewcommand{\l}{{\lambda}}
\renewcommand{\k}{{\kappa}}
\renewcommand{\L}{{\Lambda}}
\renewcommand{\t}{{\theta}}

\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\green}[1]{{\color{green} #1}}
\newcommand{\purple}[1]{{\color{purple} #1}}
\newcommand{\orange}[1]{{\color{orange} #1}}
\renewcommand{\bold}[1]{\textbf{#1}}
\renewcommand{\vec}[1]{{\overrightarrow{#1}}}
% \renewcommand{\inf}{{\infty}}

% \newenvironment{eqn}{ \begin{align} \begin{split} }{ \end{split} \end{align} }

% \newenvironment{eq}{\begin{align*}}{\end{align*}}

\newcommand{\arr}[1]{{\left[ \begin{array}{*{20}{>{\centering\arraybackslash}c}} #1 \end{array} \right]}}


\renewcommand{\table}[3]{\begin{center}\begin{tabular}{|*{#1}{>{\centering\arraybackslash}p{#2\textwidth}|}} \hline #3 \hline \end{tabular}\end{center}}

\newcommand{\tablea}[2]{\begin{center}\begin{tabular}{|*{#1}{>{\centering\arraybackslash}c|}} \hline #2 \hline \end{tabular}\end{center}}
  

% \forall \e > 0 \ \exists \d > 0 : \ \forall x \in O_\delta(x_0) \

\renewcommand{\leadsto}{{\ \Longrightarrow \ }}
\newcommand{\sat}{{\mapsto}}
\newcommand{\cons}{{\ \Rightarrow \ }}
\newcommand{\then}{{\ \Rightarrow \ }}
\newcommand{\hence}{{\ \Rightarrow \ }}


\newcommand{\thesame}{{\ \Leftrightarrow \ }}
\newcommand{\<}{\leqslant}
\renewcommand{\>}{\geqslant}
% \let\oldline\|
% \renewcommand{\#}{\oldline}
% \renewcommand{\|}{\Big|}
\renewcommand{\=}{\equiv}
\newcommand{\9}{\Big(}
\newcommand{\0}{\Big)}
\newcommand{\calc}[2]{\Bigg|_{#1}^{#2}}
\newcommand{\dx}{\, dx}
\newcommand{\dt}{\, dt}
\newcommand{\dy}{\, dy}
\newcommand{\dz}{\, dz}
\newcommand{\du}{\, du}
\newcommand{\dv}{\, dv}
\newcommand{\df}{\, d \f}
\newcommand{\bs}{\Big[}
\newcommand{\be}{\Big]}

\renewcommand{\Im}{{\operatorname{Im}}}
\renewcommand{\Re}{{\operatorname{Re}}}
\newcommand{\im}{{\operatorname{Im}}}
\newcommand{\re}{{\operatorname{Re}}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\con}{\operatorname{con}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\ifff}{\operatorname{iff}}
\newcommand{\st}{\operatorname{s.t.}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\prox}{\operatorname{prox}}

\newcommand{\else}{\text{иначе}}

\renewcommand{\exp}[1]{\operatorname{exp} \{ #1 \}}
\newcommand{\eexp}[1]{\operatorname{exp} \Big \{ #1 \Big \}}
\renewcommand{\eq}{= \\ =}
\newcommand{\shift}[1]{\Big\{ #1 \Big\}}
\newcommand{\set}[1]{\{ #1 \}}
\newcommand{\sset}[1]{\Big \{ #1 \Big \}}
$$

# Занятие 5. Лекция

План

1. Возвращаем долги
2. Тестирование гипотез

### Jackknife (=перочиный ножик)

* метод, позволяющий улучшить нахаляву оценку

На прошлом занятии обсуждали bias-variance-tradeof
$$
MSE(\hat \theta _n) = \mathbb{E} [(\hat \theta_n - \theta)^2] = \underbrace{Bias(\hat \theta_n)}_{=(\mathbb{E} \hat \theta_n - \theta)^2} + \underbrace{Var(\hat \theta_n)}_{=\mathbb{E}[(\hat \theta_n - \mathbb{E} \hat \theta_n)^2]}
$$
![image-20230331193957455](/home/alexkkir/.config/Typora/typora-user-images/image-20230331193957455.png)

* как уменьшить смещение?
  * jackknife
* как уменьшить разброс?
  * методы монте-карло

В теории хочется сделать так:
$$
\hat \theta_n^0 = \hat \theta_n - \mathbb{E} \hat \theta_nd
$$
Тогда получается $\mathbb{E} \hat \theta_n^0 = 0$, т.е. оценка несмещенная. Но как найти это матожидание?

**Пример**

![image-20230331194243904](/home/alexkkir/.config/Typora/typora-user-images/image-20230331194243904.png)

Хочется вычесть, но вычесть не можем:

![image-20230331194319762](/home/alexkkir/.config/Typora/typora-user-images/image-20230331194319762.png)

**Идея**: вычесть два раза $\theta$:
$$
\hat \theta_n^0 = \hat \theta_n - Bias(\hat \theta_n) \\
\hat \theta_n^0 - \theta= \hat \theta_n - \theta - Bias(\hat \theta_n) \quad \Big| \ \mathbb{E}[\cdot]\\
Bias(\theta_n^1) = Bias(\hat \theta_n) -Bias(\hat\theta_n) = 0
$$
Значит $\theta_n^1$ несмещенная. Но мы не знаем $Bias(\hat \theta_n)$. Поэтому берем оценку (она называется jackknife):
$$
\widehat{Bias(\hat \theta_n)} = (n - 1) \Big( {1 \over n} \sum_i^n \hat \theta_{(-i)} - \hat \theta_n\0
$$
Тогда $\mathbb{E} [\widehat{Bias(\hat \theta_n)}] = Bias(\hat \theta_n) + O({1\over n^2})$

**Пример**

![image-20230331195215833](/home/alexkkir/.config/Typora/typora-user-images/image-20230331195215833.png)

Метод jacknife:

![image-20230331195426431](/home/alexkkir/.config/Typora/typora-user-images/image-20230331195426431.png)

Получаем несмещенную оценку!!!

**Типовая ситуация**

Есть члены порядка $1/n, 1/n^2$
$$
Bias(\hat \theta_n) = {a \over n} + {b \over n^2} + \dots
$$
Тогда

![image-20230331195544802](/home/alexkkir/.config/Typora/typora-user-images/image-20230331195544802.png)

 В итоге получаем:

![image-20230331200115580](/home/alexkkir/.config/Typora/typora-user-images/image-20230331200115580.png)

В случае типовой ситуации получаем 
$$
bias(\hat \theta_n^0) = O({1 \over n^2})
$$

#  Статистические тесты

* основной инструмент в матстатистике, чтобы доказать факт по данным

### Основные вещи

Есть выборка взятая из закона распределения:
$$
X_1, \dots X_n \sim \Pp_\t
$$

Проверяем гипотезу:
$$
\Hh_0: \theta = \t_0
$$
Нужно найти $C$ - критическую область, т.ч.
$$
\P_{\t_0} \set{(X_0, \dots X_n) \in C} = \alpha = 0.05
$$
Если мы попали в это множество, то отвергаем гипотезу

### **Пример** (Нераскрытые парашуты)

Тестируем парашют с номером $i$:
$$
X_1, \dots X_n = \begin{cases} 1, & \text{парашют раскрылся},&   p = \t \\
0, & \else, & p = 1 - \t \end{cases}
$$
Гипотеза:
$$
\Hh_0: \t = 0.0001
$$
Критическое множество:
$$
C = \set{X_1 + \dots + X_n \> t_\a}
$$
(число нераскрытых парашутов больше альфа). 
$$
\P_{\t_0} \set{X_1 + \dots X_n \> t_\a} = \a \\
= \sum_{k = \lceil t_\a \rceil}^n C_n^k \t_0^k (1-\t_0)^{n - K} = \a
$$
Нужно решить это уравнения относительно $\a$. Можно с помощью R. Но есть другой способ, спомощью ЦПТ:
$$
\P_{\t_0} \set{X_1 + \dots X_n \> t_\a} = \P_{\t_0} \sset{\underbrace{{X_1 + \dots X_n - n\t_0 \over \sqrt{n \t_0 (1 - \t_0)}}}_{\sim N(0, 1)} \> {t_\a - n \t_0 \over \sqrt{n \t_0(1 - \t_0)}}} = \a
$$
Значит,
$$
{t_\a - n \t_0 \over \sqrt{n \t_0 (1 - \t_0)}} = z_{1 - \a} \hence t_\a = n\t_0 + z_{1 - \a} \sqrt{n \t_0 (1 - \t_0)}
$$
Мы считаем количество раз, сколько раскрылся парашют, с числом $t_\a$. Если больше, то отклоняем. Иначе **не отклоняется**, но не принимается.

Но тогда что мы принимаем?

Число $a$, которые было выше, называется ошибкой 1 рода. Еще есть ошибки второго рода

### Ошибки второго рода

$$
\P_{\t_0} \set{(X_0, \dots X_n) \in C} = \alpha = 0.05
$$

Идеально:
$$
\P_{\t_1} \set{(X_1, \dots, X_n) \in C} = 1 - \b
$$
где $\b$ - малое число. Оно является вероятностью ошибки второго рода.

А может ли быть так, что и $\a = \b = 0$? 

**Пример**

![image-20230331213431216](/home/alexkkir/.config/Typora/typora-user-images/image-20230331213431216.png)

Для этой задачи есть тест, для которого оба числа равны нулю

<img src="/home/alexkkir/.config/Typora/typora-user-images/image-20230331213510604.png" alt="image-20230331213510604" style="zoom:100%;" />

Тогда:

![image-20230331213557453](/home/alexkkir/.config/Typora/typora-user-images/image-20230331213557453.png)

Но это математическая экзотика

### Какой размер выборки брать для эксперимента?

Максимально прикладной вопрос (вспоминаем тинек). Можно решить с помощью чисел $\a,\b$.

**Пример про парашюты (продолжение)**

![image-20230331213756529](/home/alexkkir/.config/Typora/typora-user-images/image-20230331213756529.png)

Мы ранее вывели

![image-20230331213828428](/home/alexkkir/.config/Typora/typora-user-images/image-20230331213828428.png)

Откуда (приравниваем $t_\a$)

![image-20230331213921134](/home/alexkkir/.config/Typora/typora-user-images/image-20230331213921134.png)

### Графическая иллюстрация ошибок первого и второго рода

Функция мощности:

![image-20230331214018182](/home/alexkkir/.config/Typora/typora-user-images/image-20230331214018182.png)

Типичный график:

![image-20230331214034696](/home/alexkkir/.config/Typora/typora-user-images/image-20230331214034696.png)

Вот так выглядят ошибки первого и второго рода:

![image-20230331214157682](/home/alexkkir/.config/Typora/typora-user-images/image-20230331214157682.png)

* по параметру $ \a$ понять как делать тест
* по параметру $\b$ можем понять длякаких гипотез метод эфективный

Можно придумать тест, у которого $\a$ фиксирована, а кривая оптимальна? Да, это называется UMP tests (uniformly most powerful)

![image-20230331214508379](/home/alexkkir/.config/Typora/typora-user-images/image-20230331214508379.png)

Такие тесты можно строить благодаря теореме Неймана Пирсона

## UMP тесты

**Теорема Неймана Пирсона**

Есть две гипотезы $\t = \t_0$ и $\t = \t_1$ . Тогда следующее критическое множество дает UMP тест:
$$
C = \sset{{L_{\t_1}(X_1, \dots X_n) \over L_{\t_0}(X_1, \dots X_n)} > C_\a}
$$
где $L$ - функция правдоподобия

### Пример

![image-20230331215304116](/home/alexkkir/.config/Typora/typora-user-images/image-20230331215304116.png)

Методика построения теста:

![image-20230331215419712](/home/alexkkir/.config/Typora/typora-user-images/image-20230331215419712.png)

Тогда

![image-20230331215440309](/home/alexkkir/.config/Typora/typora-user-images/image-20230331215440309.png)

И в итоге

![image-20230331215506384](/home/alexkkir/.config/Typora/typora-user-images/image-20230331215506384.png)

### И опять экспоненциальное распределение

Оно идеально подходит под теорию.

Мы свели неравенство из теоремы НП к неравенству с суммоу $X_i$ . В случае эксп. распр. так будет всегда

Экспоненциальное семейство:

![image-20230331215750909](/home/alexkkir/.config/Typora/typora-user-images/image-20230331215750909.png)

Посчитаем:

![image-20230331215957463](/home/alexkkir/.config/Typora/typora-user-images/image-20230331215957463.png)

### Пример, когда лемма Немлана-Пирсона не работает

![image-20230331220344926](/home/alexkkir/.config/Typora/typora-user-images/image-20230331220344926.png)

Последнее уравнение решить невозможно:

  ![image-20230331220915255](/home/alexkkir/.config/Typora/typora-user-images/image-20230331220915255.png)

Получили в конце странное равенство $\hence$ тест построить не удается

Позже обсудим, что делать

### Рандомизированный статистический тест

* не является тесто v в строгом смысле этого слова

Обычный тест:
$$
\P_{\t_0} \set{(X_1,\dots X_n) \in C} = \a \\
\thesame \mathbb{E} [\I\set{(X_1, \dots X_n) \in C}]
$$
Рандомизированный тест: вводим функцию $d$:
$$
d(X_1, \dots X_n) \in [0, 1] \\
\E_\infty [d(X_1, \dots X_n)] = \a
$$
Интерпретация:

* $d(X_1, \dots X_n) = 1$ - отклоняем
* $d(X_1, \dots X_n) = 0$ - не отклоняем
* $d(X_1, \dots X_n) \in (0, 1)$ - отклоняем с вероятностью $d(X_1, \dots X_n)$

Аналог леммы Неймана-Пирсона:
$$
d(X_1, \dots X_n) = \begin{cases} 1, & {L_{\t_0}(X_1, \dots, X_n) \over L_{\t_1}(X_1, \dots, X_n)} > t_\a \\ \gamma, & (\dots) = t_\a \\ 0, & (\dots) < t_\a \end{cases}
$$
Тогда

1. $\exists ! \gamma,t_\a: \ \E_{\t_0} [d(X_1, \dots X_n)] = \a$

2. Такой тест самый мощный среди всех тестов с вероятностью ошибки первого рода $\a$, т.е. 
   $$
   \forall d^* : \E_{\t_0} [d^* (X_1, \dots X_n)] = \a \ | \ \E_\t[d^*(X_1, \dots X_n)] \underset{\forall \t}{\<}  \E_\t [d(X_1, \dots X_n)]
   $$

### Пример

![image-20230331225618940](/home/alexkkir/.config/Typora/typora-user-images/image-20230331225618940.png)

Когда $(\dots) = \a$, то используем обычную лемму НП

# LR-тесты

У нас были очень простые гипотезы: параметр равен одному числу или другому. Обычно тесты другие: параметр равен заданному числу или нет:
$$
\Hh_0: \ \theta \in \Theta_0 \sub \Theta \\
\Hh_1: \ \t \in \Theta_1 \sub \Theta
$$
Очень хочется обобщить лемму НП:

![image-20230331230041796](/home/alexkkir/.config/Typora/typora-user-images/image-20230331230041796.png)

Класс таких тестов называется LR. 

Самый важные результат в этой области:

**Теорема Уилкса**

Пусть $\Theta_0 \sub \Theta$. Тогда
$$
2 \log \9 {\max_{\theta \in \Theta} L_\theta(X_1, \dots X_n) \over \max_{\theta \in \Theta_0} L_\theta(X_1, \dots X_n)} \0 \overset{d}{\longrightarrow} \mathcal{X}^2_{\dim\Theta - \dim \Theta_0}
$$
где $X^2_p = \xi_1^2 + \xi^2_p, \ \xi_i \sim N(0, 1)$

У этой теоремы есть предположения, но опустим их (это условия регулярности). Они почти всегда выполнены.

### Пример

![image-20230331230952089](/home/alexkkir/.config/Typora/typora-user-images/image-20230331230952089.png)

В нашем примере тут вообще равенство, а не стремление

**Теорема Пирсона** (1900-ый год)

![image-20230331231610998](/home/alexkkir/.config/Typora/typora-user-images/image-20230331231610998.png)

![image-20230331231711541](/home/alexkkir/.config/Typora/typora-user-images/image-20230331231711541.png)

Решение первой проблемы - эмпирические улосвия. Решение второй проблемы (через матан):

![image-20230331231841993](/home/alexkkir/.config/Typora/typora-user-images/image-20230331231841993.png)

### Почему из теоремы Уилкса следует теорема Пирсона?

* просто полезная техника, характерная для этой области матанализа

![image-20230331233224302](/home/alexkkir/.config/Typora/typora-user-images/image-20230331233224302.png)

В конце использовали

![image-20230331233249867](/home/alexkkir/.config/Typora/typora-user-images/image-20230331233249867.png)

Ок, идем дальше. Подставляем в формулу из теоремы Уилкса:

![image-20230331233443770](/home/alexkkir/.config/Typora/typora-user-images/image-20230331233443770.png)

*Разложим в ряд Тейлора:*

![image-20230331233645994](/home/alexkkir/.config/Typora/typora-user-images/image-20230331233645994.png)

С теоремой Пирсона будет работать на семинарах.

# Критерий хи-кварат для таблиц сопряженности

Таблица сопряженности (contingency table):

![image-20230331233817343](/home/alexkkir/.config/Typora/typora-user-images/image-20230331233817343.png)

Можно было бы использовать корреляцию Пирсона. Но это предпологает, что распределение величин нормальное

![image-20230331235431651](/home/alexkkir/.config/Typora/typora-user-images/image-20230331235431651.png)

Непосредственно сам критерий:

![image-20230331235616883](/home/alexkkir/.config/Typora/typora-user-images/image-20230331235616883.png)

### **Пример**

![image-20230331235750373](/home/alexkkir/.config/Typora/typora-user-images/image-20230331235750373.png)

# Размышление про p-value

Если p-value меньше уровня значимости (например $\a = 0.05$,  $p_{value} = 0.012$), то гипотеза отвергается. Иначе, если p-value больше уровня значимости, то гипотеза *не отвергается*, но и не принимается (считаем, что не хватает данных).

![image-20230423132131349](/home/alexkkir/AIM/matstat/classes/assets/image-20230423132131349.png)

Вот снизу пример:

<img src="/home/alexkkir/AIM/matstat/classes/assets/image-20230423132231811.png" alt="image-20230423132231811" style="zoom:80%;" />

У нас получилось значение 8.61966. Значение, соответствующее p-value=0.05 равно 7.814. Наше значение находится правее порога, поэтому мы отвергаем гипотезу. 

Для p-value похожая ситуация. **p-value равно площади с правой стороны (площади правого хвоста)**. У нас она меньше 0.05$\hence$наше значение лежит правее порога, в области нереалистичных значений $\hence$гипотеза отвергается.

p-value удобнее, чем просто считать квантиль, т.к. мы сразу получаем уровень значимости. 

К слову, еще бывают двухсторонние тесты. В них мы смотрим площадь не только правого хвоста, но и левого.

### Выводы

* поговорили про тесты, рандомизированные тесты
* перешли к теореме Уилкса
  * если где-то видим сходимость к кси-квадрату, скорее всего корень лежит в этой теореме
  * поговорили про два применения
    * теорема Пирсона
    * таблица сопряженности

